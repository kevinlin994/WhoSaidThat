{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import classification_report,accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in a dataframe with previous modelling results in if it exists.\n",
    "try:\n",
    "    metrics_df = pd.read_csv('Modeling_Results')\n",
    "except:\n",
    "    metrics_df = pd.DataFrame()\n",
    "df_metrics.drop(columns=['Unnamed: 0', 'index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionary of metrics for each algorithm\n",
    "def metrics_to_dict(algorithm, y_test, y_pred, train_acc, params):\n",
    "    results = {}\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_pred, y_test)\n",
    "    recall = metrics.recall_score(y_test,y_pred)\n",
    "    f1 = metrics.f1_score(y_test,y_pred)\n",
    "    results['Algorithm'] = algorithm\n",
    "    results['Train Accuracy'] = train_acc\n",
    "    results['Hold Out Accuracy'] = accuracy\n",
    "    results['Precision'] = prec\n",
    "    results['Recall'] = recall\n",
    "    results['F1 Score'] = f1\n",
    "    results['Parameters'] = [params]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads in a dataset with te FFT info and sex of speaker\n",
    "df_fft = pd.read_csv('full_dataset_sex')\n",
    "df_fft.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fft.iloc[:,:190].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the first 25 audio samples of each speaker and averages them.\n",
    "df_mean_3 = df_fft.groupby('ID').head(25)\n",
    "df_mean_3 = df_mean_3.groupby('ID').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averages out FFT for every speaker. Reduces the number of datapoints to 5000\n",
    "mean_df = df_fft.groupby('ID').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out different numbers of PCA features\n",
    "n=25\n",
    "pca = PCA(n_components=n)\n",
    "pca_X = pca.fit_transform(df_mean_3.iloc[:,:190])\n",
    "df_features_PCA = pd.DataFrame(pca_X)\n",
    "df_features_PCA.drop(columns = ['Unnamed: 0', 'Gender'], inplace=True)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_PCA.iloc[:,193:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the X and y variables. Tested using 1 second of FFT, PCA components, and mean of FFT data.\n",
    "X = df_fft.iloc[:,:190]\n",
    "y = df_fft['Sex']\n",
    "# X = mean_df.iloc[:,:190]\n",
    "# X = df_mean_3.iloc[:,:190]\n",
    "# X = df_features_PCA\n",
    "# y = df_mean_3['Sex']\n",
    "X = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing to make sure no bugs happened\n",
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with GridSearch of hyperparameters\n",
    "log_params = {'C': [ 1e-3, 1e-2, 1e-1],\n",
    "             'penalty': ['l1','l2']}\n",
    "log = GridSearchCV(LogisticRegression(), log_params, scoring='accuracy', cv = 3)\n",
    "log.fit(X_train,y_train)\n",
    "y_scorelog=log.predict_proba(X_test)[:,1]\n",
    "y_problog = log.predict_proba(X_test)\n",
    "coefficients = list(log.best_estimator_.coef_)\n",
    "fpr_log, tpr_log,_ = roc_curve(y_test, y_scorelog)\n",
    "roc_auc = auc(fpr_log, tpr_log)\n",
    "y_pred = log.predict(X_test)\n",
    "log.score(X_train, y_train), log.score(X_test, y_test), log.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the results to a dataframe\n",
    "metrics_df = metrics_df.append(pd.DataFrame(metrics_to_dict('Logistic Regression', y_test, y_pred, log.score(X_train, y_train), log.best_params_)).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient boosting grid search of hyperparameters\n",
    "gb_param = {'n_estimators' : [100,200],\n",
    "        'max_depth' : [1,2],\n",
    "        'learning_rate': [1e-2, 1e-1, 1]}\n",
    "gradboost = GridSearchCV(GradientBoostingClassifier(), gb_param,scoring = 'accuracy', cv= 2)\n",
    "gradboost.fit(X_train, y_train)\n",
    "y_score_gboost=gradboost.predict_proba(X_test)[:,1]\n",
    "y_prob_gboost = gradboost.predict_proba(X_test)\n",
    "fpr_gboost, tpr_gboost,_ = roc_curve(y_test, y_score_gboost)\n",
    "roc_auc = auc(fpr_gboost, tpr_gboost)\n",
    "y_pred_g = gradboost.predict(X_test)\n",
    "gradboost.score(X_train, y_train), gradboost.score(X_test, y_test), gradboost.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying my classmate Chris's voice\n",
    "chris_fft = pd.read_csv('Chris_fft')\n",
    "gradboost.predict(chris_fft.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding to metrics dataframe\n",
    "metrics_df = metrics_df.append(pd.DataFrame(metrics_to_dict('Gradient Boosting w/ mean', y_test, y_pred_g, gradboost.score(X_train, y_train), gradboost.best_params_)).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried SVM with PCA components\n",
    "svm_params ={'C' : [1e-1,1,1e1,11,12,13],\n",
    "        'kernel' : ['rbf','linear'],\n",
    "        'degree': [2,3]}\n",
    "svm = GridSearchCV(SVC(), svm_params, scoring= 'accuracy', cv=3)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(svm.score(X_train, y_train)), print(svm.score(X_test, y_test)), print(svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding to results to dataframe\n",
    "metrics_df = metrics_df.append(pd.DataFrame(metrics_to_dict('SVM(PCA 25 w/mean 10)', y_test, y_pred, svm.score(X_train, y_train), svm.best_params_)).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ordering the models and hyperparamaters that yielded the highest test accuracy\n",
    "metrics_df.sort_values('Hold Out Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results dataframe\n",
    "metrics_df.to_csv('Modeling_Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried some XGBOOST but it didn't perform better than gradient boosting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {'n_estimators' : [100,200,300],\n",
    "        'max_depth' : [1,2,3,5],\n",
    "        'learning_rate': [1e-2, 1e-1, 1]}\n",
    "xgb = GridSearchCV(XGBClassifier(), xgb_param, cv=3)\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=1,learning_rate=0.1,n_estimators=380)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.append(pd.DataFrame(metrics_to_dict('XGBoost(PCA+8Features)', y_test, y_pred_xgb, xgb.score(X_train, y_train), 'None')).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
